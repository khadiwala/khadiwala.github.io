<!DOCTYPE html>
<html lang="en">
<head>
				<title>Ramblesaurus</title>
		<meta charset="utf-8" />
		<link rel="profile" href="http://gmpg.org/xfn/11" />
		<link rel="stylesheet" type="text/css" href="/theme/css/style.css" />
		<!-- Using MathJax, with the delimiters $ -->
		<!-- Conflict with pygments for the .mo and .mi -->
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
		  "HTML-CSS": {
		  styles: {
		  ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
		  },
		  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']],processEscapes: true}
		  });
		  MathJax.Hub.Register.StartupHook("HTML-CSS Jax Ready",function () {
		  var VARIANT = MathJax.OutputJax["HTML-CSS"].FONTDATA.VARIANT;
		  VARIANT["normal"].fonts.unshift("MathJax_SansSerif");
		  VARIANT["bold"].fonts.unshift("MathJax_SansSerif-bold");
		  VARIANT["italic"].fonts.unshift("MathJax_SansSerif-italic");
		  VARIANT["-tex-mathit"].fonts.unshift("MathJax_SansSerif-italic");
		  });
		  MathJax.Hub.Register.StartupHook("SVG Jax Ready",function () {
		  var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;
		  VARIANT["normal"].fonts.unshift("MathJax_SansSerif");
		  VARIANT["bold"].fonts.unshift("MathJax_SansSerif-bold");
		  VARIANT["italic"].fonts.unshift("MathJax_SansSerif-italic");
		  VARIANT["-tex-mathit"].fonts.unshift("MathJax_SansSerif-italic");
		  });
		</script>
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
		<link rel='stylesheet' id='oswald-css'  href='http://fonts.googleapis.com/css?family=Oswald&#038;ver=3.3.2' type='text/css' media='all' />
		<style type="text/css">
			body.custom-background { background-color: #f5f5f5; }
		</style>
		<link rel="alternate" type="application/atom+xml"
			title="Ramblesaurus â€” Flux Atom"
			href="/" /> 
				<!--[if lte IE 8]><script src="/theme/js/html5shiv.js"></script><![endif]-->
				</head>

<body class="home blog custom-background " >
	<div id="container">
		<div id="header">
				<h1 id="site-title"><a href="">Ramblesaurus</a></h1>
						</div><!-- /#banner -->
		
		<div id="menu">
			<div class="menu-navigation-container">
				<ul id="menu-navigation" class="menu">
										  						<li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/pages/about-me.html">"About Me"</a></li>
					  										  						<li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/category/ai.html">AI</a></li>
					  										
				</ul>
			</div> <!--/#menu-navigation-container-->
		</div><!-- /#menu -->
		
		<div class="page-title">
					</div>
	
		<div id="contents">
						<div class="post type-post status-publish format-standard hentry category-general" id="post">
	<div class="entry-meta">
		<div class="date"><a href="/thats-what-she-said.html">Fri 17 May 2013</a></div>
				<span class="byline">By <a href="/author/ravi-khadiwala.html">Ravi Khadiwala</a></span>
							<div class="comments"><a href="/thats-what-she-said.html#disqus_thread" title="Comment on That's What She Said">comments</a></div>
							<span class="cat-links"><a href="/category/ai.html" title="View all posts in AI" rel="category tag">AI</a></span>
			</div> <!-- /#entry-meta -->
	<div class="main">
		<h2 class="entry-title">
			<a href="/thats-what-she-said.html" title="Permalink to That's What She Said" rel="bookmark">That's What She Said</a>
		</h2>
		<div class="entry-content">
			<div class="ipynb"><div class="text_cell_render border-box-sizing rendered_html">
<h1 class="ipynb">That's What She Said:</h1>
<h1 class="ipynb">Automated Detection of Innuendos</h1>
<p>Here I describe my second pass at writing a classifier that can detect sentences that are likely to contain sexual inuendos. Both implementations can be found on <a href="https://github.com/mrgrieves/ThatsWhatSheSaid/tree/sk-twss">github</a>, as well as this ipython notebook (you'll need the data folder to use this notebook), which is by far the most interesting way to read this post. Although this is pretty much a joke project, since I wrote the original project I've really grown to like the <a href="http://scikit-learn.org/stable/">scikit-learn</a> library, as opposed to using the basic classifiers in nltk. I got substantially better performance and memory usage, and used a cool feature extraction technique that is well suited to bag of words models. Also I learned the wonder that is blogging with ipython notebooks, which is just too cool. Skip to the bottom to see some examples of the classifier in action, or try the old version here <a href="http://twssd.heroku.com">here</a>.</p>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h1 class="ipynb">Process the Data</h1>
<p>The data folder of the github project contains three sources. Two are of negative instances: one scraped from fmylife and the other from texts from last night. The thought here is that these samples would be written a similar tone and style to twss instances without being twss worthy. The positive samples are scraped from twssStories.com. The punctuation has already been processed out, and the data has been slightly cleaned. This <a href="http://blog.echen.me/2011/05/05/twss-building-a-thats-what-she-said-classifier/">data</a> (also availiable in my repo) was scraped by Edwin Chen, and the approach seems to be from this <a href="http://www.aclweb.org/anthology-new/P/P11/P11-2016.pdf">paper</a></p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">chain</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">load</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span>  <span class="p">:</span> <span class="nb">open</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
<span class="n">stopwords</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;english&quot;</span><span class="p">))</span>
<span class="n">stopwords</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s">&quot;twss&quot;</span><span class="p">)</span>
<span class="n">stopwords</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s">&quot;fml&quot;</span><span class="p">)</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s">&quot;data/twss-stories-parsed.txt&quot;</span><span class="p">)</span>
<span class="n">negs</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;data/fmylife-parsed.txt&quot;</span><span class="p">,</span><span class="s">&quot;data/texts-from-last-night-parsed.txt&quot;</span><span class="p">]</span>
<span class="n">neg</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">load</span><span class="p">,</span><span class="n">negs</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">tokens</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>

<span class="n">pos_f</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
<span class="n">neg_f</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">neg</span><span class="p">)</span>
<span class="n">instances</span> <span class="o">=</span> <span class="n">chain</span><span class="p">(</span><span class="n">pos_f</span><span class="p">,</span><span class="n">neg_f</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pos</span><span class="p">)),</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">neg</span><span class="p">))])</span>
</pre></div>

</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h1 class="ipynb">Feature Extraction</h1>
<h2 class="ipynb">Hash Trick</h2>
<p>Here's the more interesting part. This uses the fairly recent FeatureHasher functionality in SciKit-Learn starting with version 0.13.</p>
<p>With any text classification that has a decent size corpus, we can expect to see a huge number of features. However, each instance has very few of these features active. The first way to mitigate this effect is to use a <a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix">sparse matrix</a> as representation of the feature vectors. The additional benefit that the <a href="http://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing">hash trick</a> provides is that the hash maps directly to the indicies in the array. This means we don't have to keep an in memory mapping to feature indicies. </p>
<p>I had previously implemented this using a more straightforward approach and paid pretty large memory costs, that bottlenecked the entire process. Add to that I had not discovered the beauty of ipython notebooks and their mechanism modularizing code and only running certain cells, and I had a very slow development cycle. That's the implementation in the master branch of my twss repo. </p>
<h2 class="ipynb">Negatives</h2>
<p>You can't go backwords from the hashes to the features, so you lose the ability to easily introspect about the performance of features in the classifier. Because of this, I pulled out some info on the most informative features from when I did this experiment with nltk's implementation of naive bayes; see the end of this post (because dirty words are funny). Since this was mostly to try out different models and an interesting feature extraction technique, I did not go through the pain of mapping every single feature to it's index to find which features corresponded to the indicies that were most informative in the classifiers I tried out.</p>
<p>Potential collisions grow more likely as you approach the size of the hash space, $2^{31}$, (which I clearly do not with this data set). They are slightly mitigated by using a signed hash that determines the sign of the feature. This means collisions are more likely to cancel out than accumulate bias. It was suggested in <a href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf">Weinberger et al</a></p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">feature_extraction</span> <span class="k">as</span> <span class="n">fe</span>
<span class="n">fh</span> <span class="o">=</span> <span class="n">fe</span><span class="o">.</span><span class="n">FeatureHasher</span><span class="p">(</span><span class="n">input_type</span><span class="o">=</span><span class="s">&#39;string&#39;</span><span class="p">)</span>
<span class="n">hashed_instances</span> <span class="o">=</span> <span class="n">fh</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">instances</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h1 class="ipynb">Classification</h1>
<h2 class="ipynb">Benchmarking</h2>
<p>I pulled and modified some code to do the evaluation and timing (as well as the plotting further down), from the very good example from scikit <a href="http://scikit-learn.org/0.13/auto_examples/document_classification_20newsgroups.html">Classification of Text Documents using Sparse Features</a>. </p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>


<span class="k">def</span> <span class="nf">benchmark</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">):</span>
    <span class="n">clf_descr</span> <span class="o">=</span> <span class="n">clf</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">print</span><span class="p">(</span><span class="mi">80</span> <span class="o">*</span> <span class="s">&#39;_&#39;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;Training &quot;</span><span class="p">,</span> <span class="n">clf_descr</span><span class="p">)</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
    <span class="n">train_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;train time: </span><span class="si">%0.3f</span><span class="s">s&quot;</span> <span class="o">%</span> <span class="n">train_time</span><span class="p">)</span>

    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">])</span>
    <span class="n">test_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;test time:  </span><span class="si">%0.3f</span><span class="s">s&quot;</span> <span class="o">%</span> <span class="n">test_time</span><span class="p">)</span>

    <span class="n">score</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">pred</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;f1-score:   </span><span class="si">%0.3f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">score</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">&quot;confusion matrix:&quot;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">pred</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">clf_descr</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">train_time</span><span class="p">,</span> <span class="n">test_time</span>
</pre></div>

</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h2 class="ipynb">Models</h2>
<p>Scikit-learn is so well designed that swapping models in and out is too easy not to try. I looked for pretty much anything I could apply to a sparse matrix with negative entities. </p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Perceptron</span><span class="p">,</span> <span class="n">PassiveAggressiveClassifier</span><span class="p">,</span> <span class="n">RidgeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cross_validation</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">BernoulliNB</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NearestCentroid</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="o">.</span><span class="n">KFold</span><span class="p">(</span><span class="n">hashed_instances</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_folds</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">train</span><span class="p">,</span><span class="n">test</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">cv</span><span class="p">))</span> <span class="c"># just use one of the folds</span>

<span class="n">clss</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s">&quot;Perceptron&quot;</span><span class="p">,</span> <span class="n">Perceptron</span><span class="p">(</span><span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">&quot;BernoulliNB&quot;</span><span class="p">,</span> <span class="n">BernoulliNB</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">&quot;Ridge&quot;</span><span class="p">,</span> <span class="n">RidgeClassifier</span><span class="p">(</span><span class="n">tol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s">&quot;lsqr&quot;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">&quot;NearestCentroid&quot;</span><span class="p">,</span> <span class="n">NearestCentroid</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">&quot;LinearSVC l1&quot;</span><span class="p">,</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">&#39;l2&#39;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">&quot;l1&quot;</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">&quot;LinearSVC l2&quot;</span><span class="p">,</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">&#39;l2&#39;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">&quot;l2&quot;</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">&quot;SGD l1&quot;</span><span class="p">,</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">alpha</span><span class="o">=.</span><span class="mo">0001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">&quot;l1&quot;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">&quot;SGD l2&quot;</span><span class="p">,</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">alpha</span><span class="o">=.</span><span class="mo">0001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">&quot;l2&quot;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">&quot;SGD elasticnet&quot;</span><span class="p">,</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">alpha</span><span class="o">=.</span><span class="mo">0001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">&quot;elasticnet&quot;</span><span class="p">))</span>
       <span class="p">]</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">benchmark</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span><span class="n">hashed_instances</span><span class="p">,</span><span class="n">target</span><span class="p">,</span><span class="n">train</span><span class="p">,</span><span class="n">test</span><span class="p">)</span> <span class="k">for</span> <span class="n">cls</span> <span class="ow">in</span> <span class="n">clss</span><span class="p">]</span>
</pre></div>

</div>
</div>
<div class="vbox output_wrapper">
<div class="output vbox">
<div class="hbox output_area">
<div class="prompt output_prompt"></div>
<div class="output_subarea output_stream output_stdout">
<pre class="ipynb">________________________________________________________________________________
Training  Perceptron
train time: 0.505s
test time:  0.005s
f1-score:   0.832
confusion matrix:
[[1131   51]
 [  77  318]]
________________________________________________________________________________
Training  BernoulliNB
train time: 0.086s
test time:  0.145s
f1-score:   0.000
confusion matrix:
[[1182    0]
 [ 395    0]]
________________________________________________________________________________
Training  Ridge
train time: 48.217s
test time:  0.008s
f1-score:   0.857
confusion matrix:
[[1141   41]
 [  68  327]]
________________________________________________________________________________
Training  NearestCentroid
train time: 0.035s
test time:  0.040s
f1-score:   0.675
confusion matrix:
[[1043  139]
 [ 123  272]]
________________________________________________________________________________
Training  LinearSVC l1
train time: 0.772s
test time:  0.007s
f1-score:   0.848
confusion matrix:
[[1134   48]
 [  69  326]]
________________________________________________________________________________
Training  LinearSVC l2
train time: 3.396s
test time:  0.007s
f1-score:   0.856
confusion matrix:
[[1127   55]
 [  58  337]]
________________________________________________________________________________
Training  SGD l1
train time: 0.588s
test time:  0.006s
f1-score:   0.832
confusion matrix:
[[1125   57]
 [  73  322]]
________________________________________________________________________________
Training  SGD l2
train time: 0.524s
test time:  0.007s
f1-score:   0.842
confusion matrix:
[[1123   59]
 [  65  330]]
________________________________________________________________________________
Training  SGD elasticnet
train time: 0.581s
test time:  0.007s
f1-score:   0.836
confusion matrix:
[[1127   55]
 [  72  323]]

test time:  0.005s
f1-score:   0.832
confusion matrix:
[[1131   51]
 [  77  318]]
________________________________________________________________________________
Training  BernoulliNB
train time: 0.086s
test time:  0.145s
f1-score:   0.000
confusion matrix:
[[1182    0]
 [ 395    0]]
________________________________________________________________________________
Training  Ridge
train time: 48.217s
test time:  0.008s
f1-score:   0.857
confusion matrix:
[[1141   41]
 [  68  327]]
________________________________________________________________________________
Training  NearestCentroid
train time: 0.035s
test time:  0.040s
f1-score:   0.675
confusion matrix:
[[1043  139]
 [ 123  272]]
________________________________________________________________________________
Training  LinearSVC l1
train time: 0.772s
test time:  0.007s
f1-score:   0.848
confusion matrix:
[[1134   48]
 [  69  326]]
________________________________________________________________________________
Training  LinearSVC l2
train time: 3.396s
test time:  0.007s
f1-score:   0.856
confusion matrix:
[[1127   55]
 [  58  337]]
________________________________________________________________________________
Training  SGD l1
train time: 0.588s
test time:  0.006s
f1-score:   0.832
confusion matrix:
[[1125   57]
 [  73  322]]
________________________________________________________________________________
Training  SGD l2
train time: 0.524s
test time:  0.007s
f1-score:   0.842
confusion matrix:
[[1123   59]
 [  65  330]]
________________________________________________________________________________
Training  SGD elasticnet
train time: 0.581s
test time:  0.007s
f1-score:   0.836
confusion matrix:
[[1127   55]
 [  72  323]]
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[25]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="kn">import</span> <span class="nn">pylab</span> <span class="kn">as</span> <span class="nn">pl</span>

<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">))</span>

<span class="n">collected</span> <span class="o">=</span> <span class="p">[[</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>

<span class="n">clf_names</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">training_time</span><span class="p">,</span> <span class="n">test_time</span> <span class="o">=</span> <span class="n">collected</span>
<span class="n">training_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">training_time</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">training_time</span><span class="p">)</span>
<span class="n">test_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_time</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">test_time</span><span class="p">)</span>

<span class="n">pl</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;Score&quot;</span><span class="p">)</span>
<span class="n">pl</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;score&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
<span class="n">pl</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">indices</span> <span class="o">+</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="n">training_time</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;training time&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;g&#39;</span><span class="p">)</span>
<span class="n">pl</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">indices</span> <span class="o">+</span> <span class="o">.</span><span class="mi">6</span><span class="p">,</span> <span class="n">test_time</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;test time&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;b&#39;</span><span class="p">)</span>
<span class="n">pl</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>
<span class="n">pl</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">&#39;lower left&#39;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">))</span>
<span class="n">pl</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=.</span><span class="mi">25</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">clf_names</span><span class="p">):</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-.</span><span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>

</div>
</div>
<div class="vbox output_wrapper">
<div class="output vbox">
<div class="hbox output_area">
<div class="prompt output_prompt"></div>
<div class="output_subarea output_display_data">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAfoAAAEICAYAAAC3TzZbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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==
"></img>
</div>
</div>
</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h1 class="ipynb">That's What She Said!</h1>
<p>Great! Most of the models had similar performance, but on this data set it looks like LinearSVC performed well. Let's use this classifier to make crude jokes!</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="kn">import</span> <span class="nn">re</span>

<span class="k">def</span> <span class="nf">to_sentences</span><span class="p">(</span><span class="n">document</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s">&quot;&quot;</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">load</span><span class="p">(</span><span class="n">document</span><span class="p">):</span>
        <span class="n">text</span> <span class="o">+=</span> <span class="n">line</span>
    <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&quot;\.|\?|\!&quot;</span><span class="p">,</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">&#39;l2&#39;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">&quot;l1&quot;</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="c">#clf = SGDClassifier(alpha=.0001, n_iter=50, penalty=&quot;l1&quot;)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">hashed_instances</span><span class="p">,</span><span class="n">target</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_twss</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="n">to_sentences</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">hashed</span> <span class="o">=</span> <span class="n">fh</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span><span class="n">sentences</span><span class="p">))</span>
    <span class="n">twss_pairs</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">hashed</span><span class="p">)))</span>
    <span class="k">return</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">twss_pairs</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h2 class="ipynb">State of the Union</h2>
<p>First let's try Obama's 2013 State of the Union. Shouldn't expect too much from here, as most of these sentences will probably be non personal and contain policy related language. </p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">get_twss</span><span class="p">(</span><span class="s">&quot;data/2013-state-of-union.txt&quot;</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
</pre></div>

</div>
</div>
<div class="vbox output_wrapper">
<div class="output vbox">
<div class="hbox output_area">
<div class="prompt output_prompt"></div>
<div class="output_subarea output_stream output_stdout">
<pre class="ipynb">but we gather here knowing that there are millions of americans whose hard work and dedication have not yet been rewarded
it is our unfinished task to restore the basic bargain that built this country â€“ the idea that if you work hard and meet your responsibilities, you can get ahead, no matter where you come from, what you look like, or who you love
now, some in this congress have proposed preventing only the defense cuts by making even bigger cuts to things like education and job training; medicare and social security benefits
our government shouldnâ€™t make promises we cannot keep â€“ but we must keep the promises weâ€™ve already made
the politics will be hard for both sides
itâ€™s not a bigger government we need, but a smarter government that sets priorities and invests in broad-based growth
there are things we can do, right now, to accelerate this trend
we produce more natural gas than ever before â€“ and nearly everyoneâ€™s energy bill is lower because of it
solar energy gets cheaper by the year â€“ so letâ€™s drive costs down even further
as long as countries like china keep going all-in on clean energy, so must we
if a non-partisan coalition of ceos and retired generals and admirals can get behind this idea, then so can we
and i know that you want these job-creating projects in your districts
now, even with better high schools, most young people will need some higher education
tonight, letâ€™s also recognize that there are communities in this country where no matter how hard you work, itâ€™s virtually impossible to get ahead
letâ€™s offer incentives to companies that hire americans whoâ€™ve got what it takes to fill that job opening, but have been out of work so long that no one will give them a chance
i recognize that in our democracy, no one should just take my word that weâ€™re doing things the right way
overwhelming majorities of americans â€“ americans who believe in the 2nd amendment â€“ have come together around commonsense reform â€“ like background checks that will make it harder for criminals to get their hands on a gun
police chiefs are asking our help to get weapons of war and massive ammunition magazines off our streets, because they are tired of being outgunned
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>Hah! most of these are bad and it's not hard to see why they were misclassified, a lot of use of comparitive adjectives like bigger, harder further, etc. There are a couple good ones though, I enjoyed:</p>
<ul>
<li>
<p>the politics will be hard for both sides</p>
</li>
<li>
<p>we produce more natural gas than ever before â€“ and nearly everyoneâ€™s energy bill is lower because of it</p>
</li>
<li>
<p>there are things we can do, right now, to accelerate this trend</p>
</li>
</ul>
<p>It's a long speech with 217 sentences - if we say that none of these are good examples of that's what she said instances, this classifier was correct 197 times out of 200 (.9078). This is around the accuracy we saw on the test set (and I think some of these positive examples are correct classifications).</p>
<p>Let's see if there is anything better in George Bush's 2002 state of the union. If I remember correctly we could broach some sensitive topics, hot dog!</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[28]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">get_twss</span><span class="p">(</span><span class="s">&quot;data/2002-state-of-union.txt&quot;</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
</pre></div>

</div>
</div>
<div class="vbox output_wrapper">
<div class="output vbox">
<div class="hbox output_area">
<div class="prompt output_prompt"></div>
<div class="output_subarea output_stream output_stdout">
<pre class="ipynb">when i called our troops into action, i did so with complete confidence in their courage and skill
i don&apos;t want to play football until i can play with you again some day
so long as training camps operate, so long as nations harbor terrorists, freedom is at risk
our war on terror is well begun, but it is only begun
this campaign may not be finished on our watch â€” yet it must be and it will be waged on our watch
we can&apos;t stop short
to achieve these great national objectives â€” to win the war, protect the homeland, and revitalize our economy â€” our budget will run a deficit that will be small and short-term, so long as congress restrains spending and acts in a fiscally responsible manner
last year, some in this hall thought my tax relief plan was too small; some thought it was too big
employees who have worked hard and saved all their lives should not have to risk losing everything if their company fails
for too long our culture has said, &quot;if it feels good, do it
we&apos;ve come to know truths that we will never question: evil is real, and it must be opposed
deep in the american character, there is honor, and it is stronger than cynicism
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>Bahaha! Almost all of these are good twss candidates, or so far from it that it's pretty great. </p>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h1 class="ipynb">I Just Blue Myself</h1>
<p>Lets try some positive examples: namely some awkward quotes from Arrested Development's Tobias Funke.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[29]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="n">all_examples</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">to_sentences</span><span class="p">(</span><span class="s">&quot;data/tobias.txt&quot;</span><span class="p">))</span>
<span class="n">positive</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">get_twss</span><span class="p">(</span><span class="s">&quot;data/tobias.txt&quot;</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">&quot;twss&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">all_examples</span> <span class="o">&amp;</span> <span class="n">positive</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">negative&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">all_examples</span> <span class="o">-</span> <span class="n">positive</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
</pre></div>

</div>
</div>
<div class="vbox output_wrapper">
<div class="output vbox">
<div class="hbox output_area">
<div class="prompt output_prompt"></div>
<div class="output_subarea output_stream output_stdout">
<pre class="ipynb">twss
even if it means me taking a chubby, i will suck it up
ooh, i can taste those meaty, leading man parts in my mouth

negative
i just blue myself
i&apos;m afraid i prematurely shot my wad on what was supposed to be a dry run if you will, so i&apos;m afraid i have something of a mess on my hands
i wouldn&apos;t mind kissing that man between the cheeks
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>2 out 5 ain't terrible - I thought "I just blue myself" was a shot in the dark. Anyways the goal of these types of classifiers are to maximize preciscion rather than recall. You can easily pump many documents/examples through it, it's just important that the ones that do get positivley classified are correct. Lets try a couple more positive ones. In the spirit of "The Office" series finale, how about Michael Scott that's what she said quotes. I've grabbed a few of the top ones from <a href="http://www.reddit.com/r/DunderMifflin/comments/17zz4c/hey_guys_what_is_your_favorite_michael_scott/">this</a> reddit post, (Spoilers: and the one from the finale!)</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[30]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="n">all_examples</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">to_sentences</span><span class="p">(</span><span class="s">&quot;data/office.txt&quot;</span><span class="p">))</span>
<span class="n">positive</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">get_twss</span><span class="p">(</span><span class="s">&quot;data/office.txt&quot;</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">&quot;twss&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">all_examples</span> <span class="o">&amp;</span> <span class="n">positive</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">negative&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">all_examples</span> <span class="o">-</span> <span class="n">positive</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
</pre></div>

</div>
</div>
<div class="vbox output_wrapper">
<div class="output vbox">
<div class="hbox output_area">
<div class="prompt output_prompt"></div>
<div class="output_subarea output_stream output_stdout">
<pre class="ipynb">twss
you really think you can go all day long
wow, that is really hard
well, you always left me satisfied and smiling
my mother is coming
michael, you came

negative

no, i need two men on this
and was she under you the whole time
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>Not bad at all, 5/7 correctly identified. The first three positives are the ones that Jim softballs to Michael when he's trying get him to say it, awesome that the classifier got all of those. </p>
<p>That's about it! </p>
<p>Now, I believe I promised some dirty words ...</p>
<pre class="ipynb"><code>    contains(bigger) = True                1 : 0      =     97.0 : 1.0
      contains(suck) = True                1 : 0      =     57.1 : 1.0
     contains(shove) = True                1 : 0      =     48.9 : 1.0
   contains(sucking) = True                1 : 0      =     36.0 : 1.0
     contains(found) = True                0 : 1      =     35.3 : 1.0
       contains(wow) = True                1 : 0      =     34.5 : 1.0
    contains(sticky) = True                1 : 0      =     34.2 : 1.0
     contains(gotta) = True                1 : 0      =     32.3 : 1.0
     contains(tight) = True                1 : 0      =     29.4 : 1.0
      contains(hole) = True                1 : 0      =     29.0 : 1.0
       contains(wet) = True                1 : 0      =     27.9 : 1.0
      contains(damn) = True                1 : 0      =     26.8 : 1.0
     contains(quick) = True                1 : 0      =     24.9 : 1.0
     contains(worry) = True                1 : 0      =     24.9 : 1.0
      contains(hard) = True                1 : 0      =     24.0 : 1.0
     contains(youll) = True                1 : 0      =     22.7 : 1.0
      contains(push) = True                1 : 0      =     22.6 : 1.0
     contains(feels) = True                1 : 0      =     21.6 : 1.0
 contains(boyfriend) = True                0 : 1      =     21.6 : 1.0
    contains(harder) = True                1 : 0      =     20.4 : 1.0
      contains(year) = True                0 : 1      =     20.1 : 1.0
  contains(bathroom) = True                0 : 1      =     20.1 : 1.0
    contains(longer) = True                1 : 0      =     19.4 : 1.0
  contains(slippery) = True                1 : 0      =     19.4 : 1.0
   contains(decided) = True                0 : 1      =     19.1 : 1.0
      contains(meat) = True                1 : 0      =     18.3 : 1.0
    contains(inches) = True                1 : 0      =     17.8 : 1.0
    contains(called) = True                0 : 1      =     17.7 : 1.0
     contains(comes) = True                1 : 0      =     17.5 : 1.0
      contains(slow) = True                1 : 0      =     17.2 : 1.0
    contains(faster) = True                1 : 0      =     17.2 : 1.0
      contains(easy) = True                1 : 0      =     16.1 : 1.0
     contains(holes) = True                1 : 0      =     16.1 : 1.0
      contains(fast) = True                1 : 0      =     15.9 : 1.0
     contains(stick) = True                1 : 0      =     15.8 : 1.0
       contains(sex) = True                0 : 1      =     15.8 : 1.0
   contains(replies) = True                1 : 0      =     15.7 : 1.0
     contains(juicy) = True                1 : 0      =     15.7 : 1.0
     contains(taste) = True                1 : 0      =     15.4 : 1.0
      contains(ones) = True                1 : 0      =     15.4 : 1.0
       contains(big) = True                1 : 0      =     15.1 : 1.0
      contains(pull) = True                1 : 0      =     15.1 : 1.0
      contains(boss) = True                0 : 1      =     14.6 : 1.0
     contains(hurts) = True                1 : 0      =     14.5 : 1.0
       contains(fit) = True                1 : 0      =     14.2 : 1.0
      contains(gave) = True                0 : 1      =     14.1 : 1.0
      contains(fits) = True                1 : 0      =     13.9 : 1.0
     contains(white) = True                1 : 0      =     13.4 : 1.0
    contains(easier) = True                1 : 0      =     13.1 : 1.0
        contains(oh) = True                1 : 0      =     13.0 : 1.0
     contains(mouth) = True                1 : 0      =     13.0 : 1.0
     contains(tired) = True                1 : 0      =     12.7 : 1.0
      contains(lick) = True                1 : 0      =     12.7 : 1.0
      contains(wide) = True                1 : 0      =     12.7 : 1.0
     contains(years) = True                0 : 1      =     12.6 : 1.0
     contains(stuff) = True                1 : 0      =     12.2 : 1.0
   contains(replied) = True                1 : 0      =     12.1 : 1.0
       contains(aww) = True                1 : 0      =     12.0 : 1.0
     contains(honey) = True                1 : 0      =     12.0 : 1.0
      contains(wrap) = True                1 : 0      =     12.0 : 1.0
     contains(stiff) = True                1 : 0      =     12.0 : 1.0
   contains(shaking) = True                1 : 0      =     12.0 : 1.0
       contains(pop) = True                1 : 0      =     12.0 : 1.0
      contains(itll) = True                1 : 0      =     11.8 : 1.0
      contains(soft) = True                1 : 0      =     11.6 : 1.0
       contains(new) = True                0 : 1      =     11.6 : 1.0
     contains(slide) = True                1 : 0      =     11.5 : 1.0
      contains(door) = True                0 : 1      =     11.5 : 1.0
       contains(bar) = True                0 : 1      =     11.4 : 1.0
   contains(blowing) = True                1 : 0      =     11.4 : 1.0
      contains(home) = True                0 : 1      =     10.7 : 1.0
   contains(stretch) = True                1 : 0      =     10.5 : 1.0
       contains(cat) = True                0 : 1      =     10.5 : 1.0
      contains(shes) = True                0 : 1      =     10.5 : 1.0
    contains(window) = True                0 : 1      =     10.2 : 1.0
     contains(screw) = True                1 : 0      =     10.2 : 1.0
     contains(moist) = True                1 : 0      =     10.2 : 1.0
       contains(bet) = True                1 : 0      =     10.2 : 1.0
      contains(inch) = True                1 : 0      =     10.2 : 1.0
 contains(swallowed) = True                1 : 0      =     10.2 : 1.0
      contains(oral) = True                1 : 0      =     10.2 : 1.0
   contains(shoving) = True                1 : 0      =     10.2 : 1.0
      contains(lean) = True                1 : 0      =     10.2 : 1.0
  contains(dripping) = True                1 : 0      =     10.2 : 1.0
   contains(alright) = True                1 : 0      =     10.2 : 1.0
      contains(bend) = True                1 : 0      =     10.2 : 1.0
     contains(drunk) = True                0 : 1      =     10.0 : 1.0
       contains(omg) = True                1 : 0      =      9.8 : 1.0
contains(girlfriend) = True                0 : 1      =      9.5 : 1.0
    contains(mother) = True                0 : 1      =      9.3 : 1.0
       contains(dog) = True                0 : 1      =      9.2 : 1.0
   contains(careful) = True                1 : 0      =      9.1 : 1.0
      contains(call) = True                0 : 1      =      9.0 : 1.0
    contains(eating) = True                0 : 1      =      9.0 : 1.0
     contains(wanna) = True                1 : 0      =      8.9 : 1.0
     contains(balls) = True                1 : 0      =      8.8 : 1.0
    contains(bought) = True                0 : 1      =      8.8 : 1.0
        contains(ex) = True                0 : 1      =      8.8 : 1.0
    contains(throat) = True                1 : 0      =      8.7 : 1.0
</code></pre>
<p>This is the ranked list of the 100 most informative features when I did this experiment with naieve bayes. The first column is the feature, the second column the ratio of positives to negatives or negatives to positive with that feature present.</p>
<p>And with that, all possible remnants of humor left in the That's What She Said jokes have been taken out behind the shed and shot.</p>
</div></div>
		</div> <!--/#entry-content-->
			</div> <!--/#main-->
</div>  <!--/#post-->			<div class="post type-post status-publish format-standard hentry category-general" id="post">
	<div class="entry-meta">
		<div class="date"><a href="/rubiks-cube-solver-in-go.html">Thu 28 March 2013</a></div>
				<span class="byline">By <a href="/author/ravi-khadiwala.html">Ravi Khadiwala</a></span>
							<div class="comments"><a href="/rubiks-cube-solver-in-go.html#disqus_thread" title="Comment on "Rubiks Cube Solver in Go"">comments</a></div>
							<span class="cat-links"><a href="/category/ai.html" title="View all posts in AI" rel="category tag">AI</a></span>
			</div> <!-- /#entry-meta -->
	<div class="main">
		<h2 class="entry-title">
			<a href="/rubiks-cube-solver-in-go.html" title="Permalink to "Rubiks Cube Solver in Go"" rel="bookmark">"Rubiks Cube Solver in Go"</a>
		</h2>
		<div class="entry-content">
			<h2>Shut up about go already</h2>
<p>Despite being a few years after its initial release, I can't stop hearing about Go. 
I decided to rework a simple enough algorithm I understood to learn basic Go syntax, and see how it performs in the memory and speed departments. 
I implemented Thistlethwaite's 4 phase algorithm for solving the Rubik's Cube. I drew most of my inspiration from this fairly nice C++ <a href="http://tomas.rokicki.com/cubecontest/stefan1.txt">implementation</a> I found from Stefan Pochmann.</p>
<h3>Thistlethwaite's Algorithm</h3>
<p>The basic idea is that the search for a solved cube is broken into four smaller sequential searches. Once in a phase we limit our moves to ensure we don't exit this phase. A set of moves that can be applied to a solved cube defines a phase, the 4 phases are nested, the last one being a solved cube.</p>
<p>
$$ G_0 =  \langle U,D,L,R,F,B \rangle $$
$$ G_1 = \langle U2, D2, L, R, F,  B \rangle $$
$$ G_2 = \langle U2, D2, L, R, F2, B2 \rangle $$
$$ G_3 = \langle U2, D2, L2,R2,F2, B2 \rangle $$
$$ G_4 = \langle \rangle $$
</p>

<p>Basically a state is in phase i if, starting at the completed state, you can reach this state only by using moves in G<sub>i</sub>. I used iterative deepening DFS as opposed to BFS to save on memory at the cost of computation, since when I tried to use straight BFS I would run out of ram. At somepoint I may rewrite the cube structure to be easier on memory so I can go back to normal BFS. Another thing I'd like to do is to find a simpler representation of the cube that a human could input, instead of being forced to also input the orientations of edges and corner, but still be able to determine the phase easily based on this representation. In general the solver takes 30-40 moves to solve a randomly scrambled cube.</p>
<p><a href="http://www.jaapsch.net/puzzles/thistle.htm">Thistlethwaite's original description</a></p>
<p><a href="http://www.jaapsch.net/puzzles/compcube.htm#kocal">Kociemba's faster two phase algorithm</a></p>
<h3>Thoughts on Go</h3>
<p>Overall Go is without competition the most fun I've had with a "systems" programming language. The type inference is awesome, there are some powerful abstractions like slices, and go routines and channels are a killer language feature (which of course I did not use much of here). I think Go also captures what is most useful about object oriented programming with interfaces, without getting bogged down and forcing it on you, although the syntax is not super clear at first. I am sometimes uncomfortable with the occasional abstraction of pointers, like the automatic type conversion when you call a method for a struct its pointer. It only adds confusion, especially because you shouldn't be depending on Go for this in most cases. Most of the other things I did not enjoy were nitpicks:</p>
<ul>
<li>
<p>Compilation errors for unused variables - makes it annoying to debug pieces of code by commenting things out</p>
</li>
<li>
<p>Anonymous recursive functions must be preceded by a type definition</p>
</li>
<li>
<p>Must include return statements in functions even if unreachable</p>
</li>
</ul>
<p>My only real philisophical complaint revolve around there being a little too much magic to be a low level language, but not enough to prevent you from having to think about internals.</p>
		</div> <!--/#entry-content-->
			</div> <!--/#main-->
</div>  <!--/#post-->			<div class="post type-post status-publish format-standard hentry category-general" id="post">
	<div class="entry-meta">
		<div class="date"><a href="/Twitter based Event Detection and Analysis System.html">Sat 16 March 2013</a></div>
				<span class="byline">By <a href="/author/ravi-khadiwala.html">Ravi Khadiwala</a></span>
							<div class="comments"><a href="/Twitter based Event Detection and Analysis System.html#disqus_thread" title="Comment on "TEDAS"">comments</a></div>
							<span class="cat-links"><a href="/category/ai.html" title="View all posts in AI" rel="category tag">AI</a></span>
			</div> <!-- /#entry-meta -->
	<div class="main">
		<h2 class="entry-title">
			<a href="/Twitter based Event Detection and Analysis System.html" title="Permalink to "TEDAS"" rel="bookmark">"TEDAS"</a>
		</h2>
		<div class="entry-content">
			<h2>Twitter based Event Detection and Analysis System</h2>
<hr />
<p>Twitter is a distributed, fast, and localized system for spreading information. It's also noisy, inaccurate, and completly disorganized. 
Discovery tweets that are related to important events, for example crimes and natural disasters, and providing georgraphical context could be a useful way to propogate important news at speeds that traditional media cannot accomplish.
A demo of TEDAS is avaliable <a href="http://canary.cs.illinois.edu/crimedetection/index_ui.php">here</a>, with an assoiciated <a href="http://mias.illinois.edu/files/Twitter%20Event%20Detection%20demo%20paper.pdf">paper</a> from icde 2012.</p>
<h2>Crawling and Classification</h2>
<p>We began with a small set of seed keywords that brought in a reasonable proportion of crime related tweets. We manually labeled a few thousand of these tweets (oh god it was horrible), and had a clean dataset on which to base crawling and classification.
Based on the labeled data, keyword based textual features, and social features we created an SVM classifier. We then iterativley improve both crawling and classification by revaluating the proportion of relevant tweets that a keyword captures, removing keywords that bring in a high proportion of non event related tweets and adding keywords that are present in a high number of event related tweets. This is important because Twitter limits the number of tweets returned by its API, thus keywords that bring a lot of noise waste precious requests. Furthermore if a keyword becomes more relevant to important events or becomes co-opted by some annoying pop culture event, the system can adapt. Here is an example of finding new potential rules based on seed keywords:</p>
<table align="center"> 
<col width = "100">
<col width = "100">
        <tr>
            <td> <b> Seeds </b> </td>
            <td> <b> Discoveries </b> </td>
        </tr>
        <tr>
            <td> investigate </td>
            <td> ap breaking </td>
        </tr>
        <tr>
            <td> robbery </td>
            <td> word from </td>
        </tr>
        <tr>
            <td> arrest</td>
            <td> demonstrators </td>
        </tr> 
        <tr>
            <td> officier </td>
            <td> adventure </td>
        </tr> 
        <tr>
            <td></td>
            <td> riot </td>
        </tr>
        <tr>
            <td></td>
            <td> violate arrest </td>
        </tr>
        <tr>
            <td></td>
            <td> abducted </td>
        </tr>
        <tr>
            <td></td>
            <td> wasn't carrying drugs </td>
        </tr>
        <tr>
            <td></td>
            <td> vehicle </td>
        </tr>
</table>

<p>You can see that the discovery is highly dependant on popular subjects at the time of crawling. </p>
<p>A unique benefit to Twitter is the presence of social features. Number of followers, hashtags, retweets, all serve as useful indicators of the accuracy and relevance of tweets. Especially since traditional text based classification techniques may falter due to the short character limit of tweets (140) and the unique language tweets can take. We also can consider temporal and spatial features, like if we see several tweets at the same time and location with similar content we can infer that an important event is happening there.</p>
<table>
    <col width = "200">
    <col width = "100">
    <col width = "100">
    <col width = "100">
    <tr> 
        <td> <b> Classifier </b> </td><td><b> Accuracy </b></td><td><b> Precision </b></td><td><b> Recall </b></td>
    </tr>
    <tr> 
        <td> Text Only (Base Line) </td><td> .785924 </td><td> .818 </td><td> .606</td>
    </tr>
    <tr> 
        <td> Temporal </td><td> .798039 </td><td> .831 </td><td> .63</td>
    </tr>
    <tr> 
        <td> (Social) User Features </td><td> .797059 </td><td> .824 </td><td> .635</td>
    </tr>
    <tr> 
        <td> (Social) Tweet Features </td><td> .812745 </td><td> .805 </td><td> .71</td>
    </tr>
    <tr> 
        <td> All </td><td> .824510 </td><td> .829 </td><td> .715</td>
    </tr>
</table>

<h2>Location Resolution</h2>
<p>GPS tagged tweets are easy. Otherwise some users provide a city or state level location string in a profile, and we simply map to the center of the location. If the tweet happens to contain something that looks like an address we can pick up with a regex we use that instead.</p>
<h2>Ranking Results</h2>
<p>In order to present important tweets first in the UI we took a simple approach that proved to be effective. Pagerank like algorithms aren't practical because links between tweets aren't nessacarily common nor indicative of importance. Similarly an approach based on users connections can measure the authority of a source, but this alone is also a poor indicator of importance. 
We chose a learn a function via linear regression that assigns a real number rank to each crime related tweet based on a similar set of features we used in general classification. This combines learned importance of certain keywords, use authority, and tweet spread.</p>
<h2>Implementation</h2>
<p>We implemented TEDAS based on Java and PHP with support of MySQL, Lucene, Twitter API, and Google Maps API. At the time of writing, the system has indexed
over two million CDE tweets and about a million usersâ€™ information, and continues to index at rate of about 30,000 new CDE tweets per day. It supports detecting events related to crimes, accidents, and disasters.</p>
		</div> <!--/#entry-content-->
			</div> <!--/#main-->
</div>  <!--/#post-->		<div class="navigation">
		</div>
		</div>
		
		<div id="footer">
			<p>Powered by <a href="http://pelican.readthedocs.org">Pelican</a>, theme by <a href="http://bunnyman.info">tBunnyMan</a>.</p>
							<script type="text/javascript">
					var _gaq = _gaq || [];
					_gaq.push(['_setAccount', 'UA-39357717-1']);
					_gaq.push(['_trackPageview']);
					(function() {
						var ga = document.createElement('script'); 
						ga.type = 'text/javascript'; ga.async = true;
						ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
						var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
					})();
				</script>
										<script type="text/javascript">
    var disqus_shortname = 'ramblesaurus';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>					</div><!-- /#footer -->
	</div><!-- /#container -->
	<div style="display:none"></div>
</body>
</html>