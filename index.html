<!DOCTYPE html>
<html lang="en">
<head>
				<title>Ramblesaurus</title>
		<meta charset="utf-8" />
		<link rel="profile" href="http://gmpg.org/xfn/11" />
		<link rel="stylesheet" type="text/css" href="/theme/css/style.css" />
		<!-- Using MathJax, with the delimiters $ -->
		<!-- Conflict with pygments for the .mo and .mi -->
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
		  "HTML-CSS": {
		  styles: {
		  ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
		  },
		  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']],processEscapes: true}
		  });
		  MathJax.Hub.Register.StartupHook("HTML-CSS Jax Ready",function () {
		  var VARIANT = MathJax.OutputJax["HTML-CSS"].FONTDATA.VARIANT;
		  VARIANT["normal"].fonts.unshift("MathJax_SansSerif");
		  VARIANT["bold"].fonts.unshift("MathJax_SansSerif-bold");
		  VARIANT["italic"].fonts.unshift("MathJax_SansSerif-italic");
		  VARIANT["-tex-mathit"].fonts.unshift("MathJax_SansSerif-italic");
		  });
		  MathJax.Hub.Register.StartupHook("SVG Jax Ready",function () {
		  var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;
		  VARIANT["normal"].fonts.unshift("MathJax_SansSerif");
		  VARIANT["bold"].fonts.unshift("MathJax_SansSerif-bold");
		  VARIANT["italic"].fonts.unshift("MathJax_SansSerif-italic");
		  VARIANT["-tex-mathit"].fonts.unshift("MathJax_SansSerif-italic");
		  });
		</script>
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
		<link rel='stylesheet' id='oswald-css'  href='http://fonts.googleapis.com/css?family=Oswald&#038;ver=3.3.2' type='text/css' media='all' />
		<style type="text/css">
			body.custom-background { background-color: #f5f5f5; }
		</style>
		<link rel="alternate" type="application/atom+xml"
			title="Ramblesaurus â€” Flux Atom"
			href="/" /> 
				<!--[if lte IE 8]><script src="/theme/js/html5shiv.js"></script><![endif]-->
				</head>

<body class="home blog custom-background " >
	<div id="container">
		<div id="header">
				<h1 id="site-title"><a href="">Ramblesaurus</a></h1>
						</div><!-- /#banner -->
		
		<div id="menu">
			<div class="menu-navigation-container">
				<ul id="menu-navigation" class="menu">
										  						<li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/pages/about-me.html">"About Me"</a></li>
					  										  						<li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/category/ai.html">AI</a></li>
					  										
				</ul>
			</div> <!--/#menu-navigation-container-->
		</div><!-- /#menu -->
		
		<div class="page-title">
					</div>
	
		<div id="contents">
						<div class="post type-post status-publish format-standard hentry category-general" id="post">
	<div class="entry-meta">
		<div class="date"><a href="/thats-what-she-said.html">Fri 17 May 2013</a></div>
				<span class="byline">By <a href="/author/ravi-khadiwala.html">Ravi Khadiwala</a></span>
							<div class="comments"><a href="/thats-what-she-said.html#disqus_thread" title="Comment on That's What She Said">comments</a></div>
							<span class="cat-links"><a href="/category/ai.html" title="View all posts in AI" rel="category tag">AI</a></span>
			</div> <!-- /#entry-meta -->
	<div class="main">
		<h2 class="entry-title">
			<a href="/thats-what-she-said.html" title="Permalink to That's What She Said" rel="bookmark">That's What She Said</a>
		</h2>
		<div class="entry-content">
			<div class="ipynb"><div class="text_cell_render border-box-sizing rendered_html">
<h1 class="ipynb">That's What She Said:</h1>
<h1 class="ipynb">Automated Detection of Innuendos</h1>
<p>Here I describe my second pass at writing a classifier that can detect sentences that are likely to contain sexual inuendos. Both implementations can be found on <a href="https://github.com/mrgrieves/ThatsWhatSheSaid/tree/sk-twss">github</a>, as well as this ipython notebook (you'll need the data folder to use this notebook), which is by far the most interesting way to read this post. Although this is pretty much a joke project, since I wrote the original project I've really grown to like the <a href="http://scikit-learn.org/stable/">scikit-learn</a> library, as opposed to using the basic classifiers in nltk. I got substantially better performance and memory usage, and used a cool feature extraction technique that is well suited to bag of words models. Also I learned the wonder that is blogging with ipython notebooks, which is just too cool. Skip to the bottom to see some examples of the classifier in action, or try the old version here <a href="http://twssd.heroku.com">here</a>.</p>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h1 class="ipynb">Process the Data</h1>
<p>The data folder of the github project contains three sources. Two are of negative instances: one scraped from fmylife and the other from texts from last night. The thought here is that these samples would be written a similar tone and style to twss instances without being twss worthy. The positive samples are scraped from twssStories.com. The punctuation has already been processed out, and the data has been slightly cleaned. This <a href="http://blog.echen.me/2011/05/05/twss-building-a-thats-what-she-said-classifier/">data</a> (also availiable in my repo) was scraped by Edwin Chen, and the approach seems to be from this <a href="http://www.aclweb.org/anthology-new/P/P11/P11-2016.pdf">paper</a></p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">chain</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">load</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span>  <span class="p">:</span> <span class="nb">open</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
<span class="n">stopwords</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;english&quot;</span><span class="p">))</span>
<span class="n">stopwords</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s">&quot;twss&quot;</span><span class="p">)</span>
<span class="n">stopwords</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s">&quot;fml&quot;</span><span class="p">)</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s">&quot;data/twss-stories-parsed.txt&quot;</span><span class="p">)</span>
<span class="n">negs</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;data/fmylife-parsed.txt&quot;</span><span class="p">,</span><span class="s">&quot;data/texts-from-last-night-parsed.txt&quot;</span><span class="p">]</span>
<span class="n">neg</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">load</span><span class="p">,</span><span class="n">negs</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">tokens</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>

<span class="n">pos_f</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
<span class="n">neg_f</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">neg</span><span class="p">)</span>
<span class="n">instances</span> <span class="o">=</span> <span class="n">chain</span><span class="p">(</span><span class="n">pos_f</span><span class="p">,</span><span class="n">neg_f</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pos</span><span class="p">)),</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">neg</span><span class="p">))])</span>
</pre></div>

</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h1 class="ipynb">Feature Extraction</h1>
<h2 class="ipynb">Hash Trick</h2>
<p>Here's the more interesting part. This uses the fairly recent FeatureHasher functionality in SciKit-Learn starting with version 0.13.</p>
<p>With any text classification that has a decent size corpus, we can expect to see a huge number of features. However, each instance has very few of these features active. The first way to mitigate this effect is to use a <a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix">sparse matrix</a> as representation of the feature vectors. The additional benefit that the <a href="http://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing">hash trick</a> provides is that the hash maps directly to the indicies in the array. This means we don't have to keep an in memory mapping to feature indicies. </p>
<p>I had previously implemented this using a more straightforward approach and paid pretty large memory costs, that bottlenecked the entire process. Add to that I had not discovered the beauty of ipython notebooks and their mechanism modularizing code and only running certain cells, and I had a very slow development cycle. That's the implementation in the master branch of my twss repo. </p>
<h2 class="ipynb">Negatives</h2>
<p>You can't go backwords from the hashes to the features, so you lose the ability to easily introspect about the performance of features in the classifier. Because of this, I pulled out some info on the most informative features from when I did this experiment with nltk's implementation of naive bayes; see the end of this post (because dirty words are funny). Since this was mostly to try out different models and an interesting feature extraction technique, I did not go through the pain of mapping every single feature to it's index to find which features corresponded to the indicies that were most informative in the classifiers I tried out.</p>
<p>Potential collisions grow more likely as you approach the size of the hash space, $2^{31}$, (which I clearly do not with this data set). They are slightly mitigated by using a signed hash that determines the sign of the feature. This means collisions are more likely to cancel out than accumulate bias. It was suggested in <a href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf">Weinberger et al</a></p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">feature_extraction</span> <span class="k">as</span> <span class="n">fe</span>
<span class="n">fh</span> <span class="o">=</span> <span class="n">fe</span><span class="o">.</span><span class="n">FeatureHasher</span><span class="p">(</span><span class="n">input_type</span><span class="o">=</span><span class="s">&#39;string&#39;</span><span class="p">)</span>
<span class="n">hashed_instances</span> <span class="o">=</span> <span class="n">fh</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">instances</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h1 class="ipynb">Classification</h1>
<h2 class="ipynb">Benchmarking</h2>
<p>I pulled and modified some code to do the evaluation and timing (as well as the plotting further down), from the very good example from scikit <a href="http://scikit-learn.org/0.13/auto_examples/document_classification_20newsgroups.html">Classification of Text Documents using Sparse Features</a>. </p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>


<span class="k">def</span> <span class="nf">benchmark</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">):</span>
    <span class="n">clf_descr</span> <span class="o">=</span> <span class="n">clf</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">print</span><span class="p">(</span><span class="mi">80</span> <span class="o">*</span> <span class="s">&#39;_&#39;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;Training: &quot;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="p">)</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
    <span class="n">train_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;train time: </span><span class="si">%0.3f</span><span class="s">s&quot;</span> <span class="o">%</span> <span class="n">train_time</span><span class="p">)</span>

    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">])</span>
    <span class="n">test_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;test time:  </span><span class="si">%0.3f</span><span class="s">s&quot;</span> <span class="o">%</span> <span class="n">test_time</span><span class="p">)</span>

    <span class="n">score</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">pred</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;f1-score:   </span><span class="si">%0.3f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">score</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">&quot;confusion matrix:&quot;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">pred</span><span class="p">))</span>

    <span class="k">print</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">clf_descr</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">train_time</span><span class="p">,</span> <span class="n">test_time</span>
</pre></div>

</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h2 class="ipynb">Models</h2>
<p>Scikit-learn is so well designed that swapping models in and out is too easy not to try. I looked for pretty much anything I could apply to a sparse matrix with negative entities. </p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Perceptron</span><span class="p">,</span> <span class="n">PassiveAggressiveClassifier</span><span class="p">,</span> <span class="n">RidgeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cross_validation</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">BernoulliNB</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NearestCentroid</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="o">.</span><span class="n">KFold</span><span class="p">(</span><span class="n">hashed_instances</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_folds</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">train</span><span class="p">,</span><span class="n">test</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">cv</span><span class="p">))</span> <span class="c"># just use one of the folds</span>

<span class="n">clss</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s">&quot;Perceptron&quot;</span><span class="p">,</span> <span class="n">Perceptron</span><span class="p">(</span><span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">&quot;BernoulliNB&quot;</span><span class="p">,</span> <span class="n">BernoulliNB</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">&quot;Ridge&quot;</span><span class="p">,</span> <span class="n">RidgeClassifier</span><span class="p">(</span><span class="n">tol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s">&quot;lsqr&quot;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">&quot;NearestCentroid&quot;</span><span class="p">,</span> <span class="n">NearestCentroid</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">&quot;LinearSVC l1&quot;</span><span class="p">,</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">&#39;l2&#39;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">&quot;l1&quot;</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">&quot;LinearSVC l2&quot;</span><span class="p">,</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">&#39;l2&#39;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">&quot;l2&quot;</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">&quot;SGD l1&quot;</span><span class="p">,</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">alpha</span><span class="o">=.</span><span class="mo">0001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">&quot;l1&quot;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">&quot;SGD l2&quot;</span><span class="p">,</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">alpha</span><span class="o">=.</span><span class="mo">0001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">&quot;l2&quot;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">&quot;SGD elasticnet&quot;</span><span class="p">,</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">alpha</span><span class="o">=.</span><span class="mo">0001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">&quot;elasticnet&quot;</span><span class="p">))</span>
       <span class="p">]</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">benchmark</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span><span class="n">hashed_instances</span><span class="p">,</span><span class="n">target</span><span class="p">,</span><span class="n">train</span><span class="p">,</span><span class="n">test</span><span class="p">)</span> <span class="k">for</span> <span class="n">cls</span> <span class="ow">in</span> <span class="n">clss</span><span class="p">]</span>
</pre></div>

</div>
</div>
<div class="vbox output_wrapper">
<div class="output vbox">
<div class="hbox output_area">
<div class="prompt output_prompt"></div>
<div class="output_subarea output_stream output_stdout">
<pre class="ipynb">________________________________________________________________________________
Training: 
Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,
      n_iter=50, n_jobs=2, penalty=None, random_state=0, shuffle=False,
      verbose=0, warm_start=False)
train time: 0.558s
test time:  0.009s
f1-score:   0.843
confusion matrix:
[[1108   63]
 [  64  342]]

________________________________________________________________________________
Training: 
BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)
train time: 0.165s
test time:  0.160s
f1-score:   0.000
confusion matrix:
[[1171    0]
 [ 406    0]]

________________________________________________________________________________
Training: 
RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
        max_iter=None, normalize=False, solver=lsqr, tol=0.01)
train time: 57.789s
test time:  0.010s
f1-score:   0.843
confusion matrix:
[[1119   52]
 [  72  334]]

________________________________________________________________________________
Training: 
NearestCentroid(metric=euclidean, shrink_threshold=None)
train time: 0.043s
test time:  0.044s
f1-score:   0.654
confusion matrix:
[[1036  135]
 [ 143  263]]

________________________________________________________________________________
Training: 
LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss=l2, multi_class=ovr, penalty=l1,
     random_state=None, tol=0.001, verbose=0)
train time: 0.892s
test time:  0.008s
f1-score:   0.848
confusion matrix:
[[1122   49]
 [  71  335]]

________________________________________________________________________________
Training: 
LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss=l2, multi_class=ovr, penalty=l2,
     random_state=None, tol=0.001, verbose=0)
train time: 4.923s
test time:  0.006s
f1-score:   0.856
confusion matrix:
[[1120   51]
 [  64  342]]

________________________________________________________________________________
Training: 
SGDClassifier(alpha=0.0001, class_weight=None, epsilon=0.1, eta0=0.0,
       fit_intercept=True, l1_ratio=0.15, learning_rate=optimal,
       loss=hinge, n_iter=50, n_jobs=1, penalty=l1, power_t=0.5,
       random_state=None, rho=None, shuffle=False, verbose=0,
       warm_start=False)
train time: 0.720s
test time:  0.008s
f1-score:   0.823
confusion matrix:
[[1105   66]
 [  76  330]]

________________________________________________________________________________
Training: 
SGDClassifier(alpha=0.0001, class_weight=None, epsilon=0.1, eta0=0.0,
       fit_intercept=True, l1_ratio=0.15, learning_rate=optimal,
       loss=hinge, n_iter=50, n_jobs=1, penalty=l2, power_t=0.5,
       random_state=None, rho=None, shuffle=False, verbose=0,
       warm_start=False)
train time: 0.659s
test time:  0.006s
f1-score:   0.855
confusion matrix:
[[1119   52]
 [  64  342]]

________________________________________________________________________________
Training: 
SGDClassifier(alpha=0.0001, class_weight=None, epsilon=0.1, eta0=0.0,
       fit_intercept=True, l1_ratio=0.15, learning_rate=optimal,
       loss=hinge, n_iter=50, n_jobs=1, penalty=elasticnet, power_t=0.5,
       random_state=None, rho=None, shuffle=False, verbose=0,
       warm_start=False)
train time: 0.726s
test time:  0.008s
f1-score:   0.844
confusion matrix:
[[1122   49]
 [  74  332]]


test time:  0.009s
f1-score:   0.843
confusion matrix:
[[1108   63]
 [  64  342]]

________________________________________________________________________________
Training: 
BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)
train time: 0.165s
test time:  0.160s
f1-score:   0.000
confusion matrix:
[[1171    0]
 [ 406    0]]

________________________________________________________________________________
Training: 
RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
        max_iter=None, normalize=False, solver=lsqr, tol=0.01)
train time: 57.789s
test time:  0.010s
f1-score:   0.843
confusion matrix:
[[1119   52]
 [  72  334]]

________________________________________________________________________________
Training: 
NearestCentroid(metric=euclidean, shrink_threshold=None)
train time: 0.043s
test time:  0.044s
f1-score:   0.654
confusion matrix:
[[1036  135]
 [ 143  263]]

________________________________________________________________________________
Training: 
LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss=l2, multi_class=ovr, penalty=l1,
     random_state=None, tol=0.001, verbose=0)
train time: 0.892s
test time:  0.008s
f1-score:   0.848
confusion matrix:
[[1122   49]
 [  71  335]]

________________________________________________________________________________
Training: 
LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss=l2, multi_class=ovr, penalty=l2,
     random_state=None, tol=0.001, verbose=0)
train time: 4.923s
test time:  0.006s
f1-score:   0.856
confusion matrix:
[[1120   51]
 [  64  342]]

________________________________________________________________________________
Training: 
SGDClassifier(alpha=0.0001, class_weight=None, epsilon=0.1, eta0=0.0,
       fit_intercept=True, l1_ratio=0.15, learning_rate=optimal,
       loss=hinge, n_iter=50, n_jobs=1, penalty=l1, power_t=0.5,
       random_state=None, rho=None, shuffle=False, verbose=0,
       warm_start=False)
train time: 0.720s
test time:  0.008s
f1-score:   0.823
confusion matrix:
[[1105   66]
 [  76  330]]

________________________________________________________________________________
Training: 
SGDClassifier(alpha=0.0001, class_weight=None, epsilon=0.1, eta0=0.0,
       fit_intercept=True, l1_ratio=0.15, learning_rate=optimal,
       loss=hinge, n_iter=50, n_jobs=1, penalty=l2, power_t=0.5,
       random_state=None, rho=None, shuffle=False, verbose=0,
       warm_start=False)
train time: 0.659s
test time:  0.006s
f1-score:   0.855
confusion matrix:
[[1119   52]
 [  64  342]]

________________________________________________________________________________
Training: 
SGDClassifier(alpha=0.0001, class_weight=None, epsilon=0.1, eta0=0.0,
       fit_intercept=True, l1_ratio=0.15, learning_rate=optimal,
       loss=hinge, n_iter=50, n_jobs=1, penalty=elasticnet, power_t=0.5,
       random_state=None, rho=None, shuffle=False, verbose=0,
       warm_start=False)
train time: 0.726s
test time:  0.008s
f1-score:   0.844
confusion matrix:
[[1122   49]
 [  74  332]]

</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="kn">import</span> <span class="nn">pylab</span> <span class="kn">as</span> <span class="nn">pl</span>

<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">))</span>

<span class="n">collected</span> <span class="o">=</span> <span class="p">[[</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>

<span class="n">clf_names</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">training_time</span><span class="p">,</span> <span class="n">test_time</span> <span class="o">=</span> <span class="n">collected</span>
<span class="n">training_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">training_time</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">training_time</span><span class="p">)</span>
<span class="n">test_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_time</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">test_time</span><span class="p">)</span>

<span class="n">pl</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;Score&quot;</span><span class="p">)</span>
<span class="n">pl</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;score&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
<span class="n">pl</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">indices</span> <span class="o">+</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="n">training_time</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;training time&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;g&#39;</span><span class="p">)</span>
<span class="n">pl</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">indices</span> <span class="o">+</span> <span class="o">.</span><span class="mi">6</span><span class="p">,</span> <span class="n">test_time</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;test time&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;b&#39;</span><span class="p">)</span>
<span class="n">pl</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>
<span class="n">pl</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">&#39;lower left&#39;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">))</span>
<span class="n">pl</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=.</span><span class="mi">25</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">clf_names</span><span class="p">):</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-.</span><span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>

</div>
</div>
<div class="vbox output_wrapper">
<div class="output vbox">
<div class="hbox output_area">
<div class="prompt output_prompt"></div>
<div class="output_subarea output_display_data">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAfoAAAEHCAYAAABGGYSOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"></img>
</div>
</div>
</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h1 class="ipynb">That's What She Said!</h1>
<p>Great! Most of the models had similar performance, but on this data set it looks like LinearSVC performed well. Let's use this classifier to make crude jokes!</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="kn">import</span> <span class="nn">re</span>

<span class="k">def</span> <span class="nf">to_sentences</span><span class="p">(</span><span class="n">document</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s">&quot;&quot;</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">load</span><span class="p">(</span><span class="n">document</span><span class="p">):</span>
        <span class="n">text</span> <span class="o">+=</span> <span class="n">line</span>
    <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&quot;\.|\?|\!&quot;</span><span class="p">,</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">&#39;l2&#39;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">&quot;l1&quot;</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="c">#clf = SGDClassifier(alpha=.0001, n_iter=50, penalty=&quot;l1&quot;)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">hashed_instances</span><span class="p">,</span><span class="n">target</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_twss</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="n">to_sentences</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">hashed</span> <span class="o">=</span> <span class="n">fh</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span><span class="n">sentences</span><span class="p">))</span>
    <span class="n">twss_pairs</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">hashed</span><span class="p">)))</span>
    <span class="k">return</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">twss_pairs</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h2 class="ipynb">State of the Union</h2>
<p>First let's try Obama's 2013 State of the Union. Shouldn't expect too much from here, as most of these sentences will probably be non personal and contain policy related language. </p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[43]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">get_twss</span><span class="p">(</span><span class="s">&quot;data/2013-state-of-union.txt&quot;</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
</pre></div>

</div>
</div>
<div class="vbox output_wrapper">
<div class="output vbox">
<div class="hbox output_area">
<div class="prompt output_prompt"></div>
<div class="output_subarea output_stream output_stdout">
<pre class="ipynb">but we gather here knowing that there are millions of americans whose hard work and dedication have not yet been rewarded
it is our unfinished task to restore the basic bargain that built this country â€“ the idea that if you work hard and meet your responsibilities, you can get ahead, no matter where you come from, what you look like, or who you love
now, some in this congress have proposed preventing only the defense cuts by making even bigger cuts to things like education and job training; medicare and social security benefits
our government shouldnâ€™t make promises we cannot keep â€“ but we must keep the promises weâ€™ve already made
the politics will be hard for both sides
itâ€™s not a bigger government we need, but a smarter government that sets priorities and invests in broad-based growth
there are things we can do, right now, to accelerate this trend
we produce more natural gas than ever before â€“ and nearly everyoneâ€™s energy bill is lower because of it
solar energy gets cheaper by the year â€“ so letâ€™s drive costs down even further
as long as countries like china keep going all-in on clean energy, so must we
if a non-partisan coalition of ceos and retired generals and admirals can get behind this idea, then so can we
and i know that you want these job-creating projects in your districts
now, even with better high schools, most young people will need some higher education
tonight, letâ€™s also recognize that there are communities in this country where no matter how hard you work, itâ€™s virtually impossible to get ahead
letâ€™s offer incentives to companies that hire americans whoâ€™ve got what it takes to fill that job opening, but have been out of work so long that no one will give them a chance
stronger families
stronger communities
i recognize that in our democracy, no one should just take my word that weâ€™re doing things the right way
overwhelming majorities of americans â€“ americans who believe in the 2nd amendment â€“ have come together around commonsense reform â€“ like background checks that will make it harder for criminals to get their hands on a gun
police chiefs are asking our help to get weapons of war and massive ammunition magazines off our streets, because they are tired of being outgunned
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>Hah! most of these are bad and it's not hard to see why they were misclassified, a lot of use of comparitive adjectives like bigger, harder further, etc. There are a couple good ones though, I enjoyed:</p>
<ul>
<li>
<p>the politics will be hard for both sides</p>
</li>
<li>
<p>we produce more natural gas than ever before â€“ and nearly everyoneâ€™s energy bill is lower because of it</p>
</li>
<li>
<p>there are things we can do, right now, to accelerate this trend</p>
</li>
</ul>
<p>It's a long speech with 217 sentences - if we say that none of these are good examples of that's what she said instances, this classifier was correct 197 times out of 200 (.9078). This is around the accuracy we saw on the test set (and I think some of these positive examples are correct classifications).</p>
<p>Let's see if there is anything better in George Bush's 2002 state of the union. If I remember correctly we could broach some sensitive topics, hot dog!</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[28]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">get_twss</span><span class="p">(</span><span class="s">&quot;data/2002-state-of-union.txt&quot;</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
</pre></div>

</div>
</div>
<div class="vbox output_wrapper">
<div class="output vbox">
<div class="hbox output_area">
<div class="prompt output_prompt"></div>
<div class="output_subarea output_stream output_stdout">
<pre class="ipynb">when i called our troops into action, i did so with complete confidence in their courage and skill
i don&apos;t want to play football until i can play with you again some day
so long as training camps operate, so long as nations harbor terrorists, freedom is at risk
our war on terror is well begun, but it is only begun
this campaign may not be finished on our watch â€” yet it must be and it will be waged on our watch
we can&apos;t stop short
last year, some in this hall thought my tax relief plan was too small; some thought it was too big
employees who have worked hard and saved all their lives should not have to risk losing everything if their company fails
for too long our culture has said, &quot;if it feels good, do it
we&apos;ve come to know truths that we will never question: evil is real, and it must be opposed
deep in the american character, there is honor, and it is stronger than cynicism
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>Bahaha! Almost all of these are good twss candidates, or so far from it that it's pretty great. </p>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h1 class="ipynb">I Just Blue Myself</h1>
<p>Lets try some positive examples: namely some awkward quotes from Arrested Development's Tobias Funke.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[39]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="n">all_examples</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">to_sentences</span><span class="p">(</span><span class="s">&quot;data/tobias.txt&quot;</span><span class="p">))</span>
<span class="n">positive</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">get_twss</span><span class="p">(</span><span class="s">&quot;data/tobias.txt&quot;</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">&quot;twss&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">all_examples</span> <span class="o">&amp;</span> <span class="n">positive</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">negative&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">all_examples</span> <span class="o">-</span> <span class="n">positive</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
</pre></div>

</div>
</div>
<div class="vbox output_wrapper">
<div class="output vbox">
<div class="hbox output_area">
<div class="prompt output_prompt"></div>
<div class="output_subarea output_stream output_stdout">
<pre class="ipynb">twss
even if it means me taking a chubby, i will suck it up
ooh, i can taste those meaty, leading man parts in my mouth

negative
i just blue myself
i&apos;m afraid i prematurely shot my wad on what was supposed to be a dry run if you will, so i&apos;m afraid i have something of a mess on my hands
i wouldn&apos;t mind kissing that man between the cheeks
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>2 out 5 ain't terrible - I thought "I just blue myself" was a shot in the dark. Anyways the goal of these types of classifiers are to maximize preciscion rather than recall. You can easily pump many documents/examples through it, it's just important that the ones that do get positivley classified are correct. Lets try a couple more positive ones. In the spirit of "The Office" series finale, how about Michael Scott that's what she said quotes. I've grabbed a few of the top ones from <a href="http://www.reddit.com/r/DunderMifflin/comments/17zz4c/hey_guys_what_is_your_favorite_michael_scott/">this</a> reddit post, (Spoilers: and the one from the finale!)</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[47]:</div>
<div class="input_area box-flex1">
<div class="highlight-ipynb"><pre class="ipynb"><span class="n">all_examples</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">to_sentences</span><span class="p">(</span><span class="s">&quot;data/office.txt&quot;</span><span class="p">))</span>
<span class="n">positive</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">get_twss</span><span class="p">(</span><span class="s">&quot;data/office.txt&quot;</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">&quot;twss&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">all_examples</span> <span class="o">&amp;</span> <span class="n">positive</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">negative&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">all_examples</span> <span class="o">-</span> <span class="n">positive</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
</pre></div>

</div>
</div>
<div class="vbox output_wrapper">
<div class="output vbox">
<div class="hbox output_area">
<div class="prompt output_prompt"></div>
<div class="output_subarea output_stream output_stdout">
<pre class="ipynb">twss
you really think you can go all day long
wow, that is really hard
well, you always left me satisfied and smiling
my mother is coming
michael, you came

negative

and was she under you the whole time
no, i need two men on this
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>Not bad at all, 5/7 correctly identified. The first three positives are the ones that Jim softballs to Michael when he's trying get him to say it, awesome that the classifier got all of those. </p>
<p>That's about it! </p>
<p>Now, I believe I promised some dirty words ...</p>
<pre class="ipynb"><code>    contains(bigger) = True                1 : 0      =     97.0 : 1.0
      contains(suck) = True                1 : 0      =     57.1 : 1.0
     contains(shove) = True                1 : 0      =     48.9 : 1.0
   contains(sucking) = True                1 : 0      =     36.0 : 1.0
     contains(found) = True                0 : 1      =     35.3 : 1.0
       contains(wow) = True                1 : 0      =     34.5 : 1.0
    contains(sticky) = True                1 : 0      =     34.2 : 1.0
     contains(gotta) = True                1 : 0      =     32.3 : 1.0
     contains(tight) = True                1 : 0      =     29.4 : 1.0
      contains(hole) = True                1 : 0      =     29.0 : 1.0
       contains(wet) = True                1 : 0      =     27.9 : 1.0
      contains(damn) = True                1 : 0      =     26.8 : 1.0
     contains(quick) = True                1 : 0      =     24.9 : 1.0
     contains(worry) = True                1 : 0      =     24.9 : 1.0
      contains(hard) = True                1 : 0      =     24.0 : 1.0
     contains(youll) = True                1 : 0      =     22.7 : 1.0
      contains(push) = True                1 : 0      =     22.6 : 1.0
     contains(feels) = True                1 : 0      =     21.6 : 1.0
 contains(boyfriend) = True                0 : 1      =     21.6 : 1.0
    contains(harder) = True                1 : 0      =     20.4 : 1.0
      contains(year) = True                0 : 1      =     20.1 : 1.0
  contains(bathroom) = True                0 : 1      =     20.1 : 1.0
    contains(longer) = True                1 : 0      =     19.4 : 1.0
  contains(slippery) = True                1 : 0      =     19.4 : 1.0
   contains(decided) = True                0 : 1      =     19.1 : 1.0
      contains(meat) = True                1 : 0      =     18.3 : 1.0
    contains(inches) = True                1 : 0      =     17.8 : 1.0
    contains(called) = True                0 : 1      =     17.7 : 1.0
     contains(comes) = True                1 : 0      =     17.5 : 1.0
      contains(slow) = True                1 : 0      =     17.2 : 1.0
    contains(faster) = True                1 : 0      =     17.2 : 1.0
      contains(easy) = True                1 : 0      =     16.1 : 1.0
     contains(holes) = True                1 : 0      =     16.1 : 1.0
      contains(fast) = True                1 : 0      =     15.9 : 1.0
     contains(stick) = True                1 : 0      =     15.8 : 1.0
       contains(sex) = True                0 : 1      =     15.8 : 1.0
   contains(replies) = True                1 : 0      =     15.7 : 1.0
     contains(juicy) = True                1 : 0      =     15.7 : 1.0
     contains(taste) = True                1 : 0      =     15.4 : 1.0
      contains(ones) = True                1 : 0      =     15.4 : 1.0
       contains(big) = True                1 : 0      =     15.1 : 1.0
      contains(pull) = True                1 : 0      =     15.1 : 1.0
      contains(boss) = True                0 : 1      =     14.6 : 1.0
     contains(hurts) = True                1 : 0      =     14.5 : 1.0
       contains(fit) = True                1 : 0      =     14.2 : 1.0
      contains(gave) = True                0 : 1      =     14.1 : 1.0
      contains(fits) = True                1 : 0      =     13.9 : 1.0
     contains(white) = True                1 : 0      =     13.4 : 1.0
    contains(easier) = True                1 : 0      =     13.1 : 1.0
        contains(oh) = True                1 : 0      =     13.0 : 1.0
     contains(mouth) = True                1 : 0      =     13.0 : 1.0
     contains(tired) = True                1 : 0      =     12.7 : 1.0
      contains(lick) = True                1 : 0      =     12.7 : 1.0
      contains(wide) = True                1 : 0      =     12.7 : 1.0
     contains(years) = True                0 : 1      =     12.6 : 1.0
     contains(stuff) = True                1 : 0      =     12.2 : 1.0
   contains(replied) = True                1 : 0      =     12.1 : 1.0
       contains(aww) = True                1 : 0      =     12.0 : 1.0
     contains(honey) = True                1 : 0      =     12.0 : 1.0
      contains(wrap) = True                1 : 0      =     12.0 : 1.0
     contains(stiff) = True                1 : 0      =     12.0 : 1.0
   contains(shaking) = True                1 : 0      =     12.0 : 1.0
       contains(pop) = True                1 : 0      =     12.0 : 1.0
      contains(itll) = True                1 : 0      =     11.8 : 1.0
      contains(soft) = True                1 : 0      =     11.6 : 1.0
       contains(new) = True                0 : 1      =     11.6 : 1.0
     contains(slide) = True                1 : 0      =     11.5 : 1.0
      contains(door) = True                0 : 1      =     11.5 : 1.0
       contains(bar) = True                0 : 1      =     11.4 : 1.0
   contains(blowing) = True                1 : 0      =     11.4 : 1.0
      contains(home) = True                0 : 1      =     10.7 : 1.0
   contains(stretch) = True                1 : 0      =     10.5 : 1.0
       contains(cat) = True                0 : 1      =     10.5 : 1.0
      contains(shes) = True                0 : 1      =     10.5 : 1.0
    contains(window) = True                0 : 1      =     10.2 : 1.0
     contains(screw) = True                1 : 0      =     10.2 : 1.0
     contains(moist) = True                1 : 0      =     10.2 : 1.0
       contains(bet) = True                1 : 0      =     10.2 : 1.0
      contains(inch) = True                1 : 0      =     10.2 : 1.0
 contains(swallowed) = True                1 : 0      =     10.2 : 1.0
      contains(oral) = True                1 : 0      =     10.2 : 1.0
   contains(shoving) = True                1 : 0      =     10.2 : 1.0
      contains(lean) = True                1 : 0      =     10.2 : 1.0
  contains(dripping) = True                1 : 0      =     10.2 : 1.0
   contains(alright) = True                1 : 0      =     10.2 : 1.0
      contains(bend) = True                1 : 0      =     10.2 : 1.0
     contains(drunk) = True                0 : 1      =     10.0 : 1.0
       contains(omg) = True                1 : 0      =      9.8 : 1.0
contains(girlfriend) = True                0 : 1      =      9.5 : 1.0
    contains(mother) = True                0 : 1      =      9.3 : 1.0
       contains(dog) = True                0 : 1      =      9.2 : 1.0
   contains(careful) = True                1 : 0      =      9.1 : 1.0
      contains(call) = True                0 : 1      =      9.0 : 1.0
    contains(eating) = True                0 : 1      =      9.0 : 1.0
     contains(wanna) = True                1 : 0      =      8.9 : 1.0
     contains(balls) = True                1 : 0      =      8.8 : 1.0
    contains(bought) = True                0 : 1      =      8.8 : 1.0
        contains(ex) = True                0 : 1      =      8.8 : 1.0
    contains(throat) = True                1 : 0      =      8.7 : 1.0
</code></pre>
<p>This is the ranked list of the 100 most informative features when I did this experiment with naieve bayes. The first column is the feature, the second column the ratio of positives to negatives or negatives to positive with that feature present.</p>
<p>And with that, all possible remnants of humor left in the That's What She Said jokes have been taken out behind the shed and shot.</p>
</div></div>
		</div> <!--/#entry-content-->
			</div> <!--/#main-->
</div>  <!--/#post-->			<div class="post type-post status-publish format-standard hentry category-general" id="post">
	<div class="entry-meta">
		<div class="date"><a href="/rubiks-cube-solver-in-go.html">Thu 28 March 2013</a></div>
				<span class="byline">By <a href="/author/ravi-khadiwala.html">Ravi Khadiwala</a></span>
							<div class="comments"><a href="/rubiks-cube-solver-in-go.html#disqus_thread" title="Comment on "Rubiks Cube Solver in Go"">comments</a></div>
							<span class="cat-links"><a href="/category/ai.html" title="View all posts in AI" rel="category tag">AI</a></span>
			</div> <!-- /#entry-meta -->
	<div class="main">
		<h2 class="entry-title">
			<a href="/rubiks-cube-solver-in-go.html" title="Permalink to "Rubiks Cube Solver in Go"" rel="bookmark">"Rubiks Cube Solver in Go"</a>
		</h2>
		<div class="entry-content">
			<h2>Shut up about go already</h2>
<p>Despite being a few years after its initial release, I can't stop hearing about Go. 
I decided to rework a simple enough algorithm I understood to learn basic Go syntax, and see how it performs in the memory and speed departments. 
I implemented Thistlethwaite's 4 phase algorithm for solving the Rubik's Cube. I drew most of my inspiration from this fairly nice C++ <a href="http://tomas.rokicki.com/cubecontest/stefan1.txt">implementation</a> I found from Stefan Pochmann.</p>
<h3>Thistlethwaite's Algorithm</h3>
<p>The basic idea is that the search for a solved cube is broken into four smaller sequential searches. Once in a phase we limit our moves to ensure we don't exit this phase. A set of moves that can be applied to a solved cube defines a phase, the 4 phases are nested, the last one being a solved cube.</p>
<p>
$$ G_0 =  \langle U,D,L,R,F,B \rangle $$
$$ G_1 = \langle U2, D2, L, R, F,  B \rangle $$
$$ G_2 = \langle U2, D2, L, R, F2, B2 \rangle $$
$$ G_3 = \langle U2, D2, L2,R2,F2, B2 \rangle $$
$$ G_4 = \langle \rangle $$
</p>

<p>Basically a state is in phase i if, starting at the completed state, you can reach this state only by using moves in G<sub>i</sub>. I used iterative deepening DFS as opposed to BFS to save on memory at the cost of computation, since when I tried to use straight BFS I would run out of ram. At somepoint I may rewrite the cube structure to be easier on memory so I can go back to normal BFS. Another thing I'd like to do is to find a simpler representation of the cube that a human could input, instead of being forced to also input the orientations of edges and corner, but still be able to determine the phase easily based on this representation. In general the solver takes 30-40 moves to solve a randomly scrambled cube.</p>
<p><a href="http://www.jaapsch.net/puzzles/thistle.htm">Thistlethwaite's original description</a></p>
<p><a href="http://www.jaapsch.net/puzzles/compcube.htm#kocal">Kociemba's faster two phase algorithm</a></p>
<h3>Thoughts on Go</h3>
<p>Overall Go is without competition the most fun I've had with a "systems" programming language. The type inference is awesome, there are some powerful abstractions like slices, and go routines and channels are a killer language feature (which of course I did not use much of here). I think Go also captures what is most useful about object oriented programming with interfaces, without getting bogged down and forcing it on you, although the syntax is not super clear at first. I am sometimes uncomfortable with the occasional abstraction of pointers, like the automatic type conversion when you call a method for a struct its pointer. It only adds confusion, especially because you shouldn't be depending on Go for this in most cases. Most of the other things I did not enjoy were nitpicks:</p>
<ul>
<li>
<p>Compilation errors for unused variables - makes it annoying to debug pieces of code by commenting things out</p>
</li>
<li>
<p>Anonymous recursive functions must be preceded by a type definition</p>
</li>
<li>
<p>Must include return statements in functions even if unreachable</p>
</li>
</ul>
<p>My only real philisophical complaint revolve around there being a little too much magic to be a low level language, but not enough to prevent you from having to think about internals.</p>
		</div> <!--/#entry-content-->
			</div> <!--/#main-->
</div>  <!--/#post-->			<div class="post type-post status-publish format-standard hentry category-general" id="post">
	<div class="entry-meta">
		<div class="date"><a href="/Twitter based Event Detection and Analysis System.html">Sat 16 March 2013</a></div>
				<span class="byline">By <a href="/author/ravi-khadiwala.html">Ravi Khadiwala</a></span>
							<div class="comments"><a href="/Twitter based Event Detection and Analysis System.html#disqus_thread" title="Comment on "TEDAS"">comments</a></div>
							<span class="cat-links"><a href="/category/ai.html" title="View all posts in AI" rel="category tag">AI</a></span>
			</div> <!-- /#entry-meta -->
	<div class="main">
		<h2 class="entry-title">
			<a href="/Twitter based Event Detection and Analysis System.html" title="Permalink to "TEDAS"" rel="bookmark">"TEDAS"</a>
		</h2>
		<div class="entry-content">
			<h2>Twitter based Event Detection and Analysis System</h2>
<hr />
<p>Twitter is a distributed, fast, and localized system for spreading information. It's also noisy, inaccurate, and completly disorganized. 
Discovery tweets that are related to important events, for example crimes and natural disasters, and providing georgraphical context could be a useful way to propogate important news at speeds that traditional media cannot accomplish.
A demo of TEDAS is avaliable <a href="http://canary.cs.illinois.edu/crimedetection/index_ui.php">here</a>, with an assoiciated <a href="http://mias.illinois.edu/files/Twitter%20Event%20Detection%20demo%20paper.pdf">paper</a> from icde 2012.</p>
<h2>Crawling and Classification</h2>
<p>We began with a small set of seed keywords that brought in a reasonable proportion of crime related tweets. We manually labeled a few thousand of these tweets (oh god it was horrible), and had a clean dataset on which to base crawling and classification.
Based on the labeled data, keyword based textual features, and social features we created an SVM classifier. We then iterativley improve both crawling and classification by revaluating the proportion of relevant tweets that a keyword captures, removing keywords that bring in a high proportion of non event related tweets and adding keywords that are present in a high number of event related tweets. This is important because Twitter limits the number of tweets returned by its API, thus keywords that bring a lot of noise waste precious requests. Furthermore if a keyword becomes more relevant to important events or becomes co-opted by some annoying pop culture event, the system can adapt. Here is an example of finding new potential rules based on seed keywords:</p>
<table align="center"> 
<col width = "100">
<col width = "100">
        <tr>
            <td> <b> Seeds </b> </td>
            <td> <b> Discoveries </b> </td>
        </tr>
        <tr>
            <td> investigate </td>
            <td> ap breaking </td>
        </tr>
        <tr>
            <td> robbery </td>
            <td> word from </td>
        </tr>
        <tr>
            <td> arrest</td>
            <td> demonstrators </td>
        </tr> 
        <tr>
            <td> officier </td>
            <td> adventure </td>
        </tr> 
        <tr>
            <td></td>
            <td> riot </td>
        </tr>
        <tr>
            <td></td>
            <td> violate arrest </td>
        </tr>
        <tr>
            <td></td>
            <td> abducted </td>
        </tr>
        <tr>
            <td></td>
            <td> wasn't carrying drugs </td>
        </tr>
        <tr>
            <td></td>
            <td> vehicle </td>
        </tr>
</table>

<p>You can see that the discovery is highly dependant on popular subjects at the time of crawling. </p>
<p>A unique benefit to Twitter is the presence of social features. Number of followers, hashtags, retweets, all serve as useful indicators of the accuracy and relevance of tweets. Especially since traditional text based classification techniques may falter due to the short character limit of tweets (140) and the unique language tweets can take. We also can consider temporal and spatial features, like if we see several tweets at the same time and location with similar content we can infer that an important event is happening there.</p>
<table>
    <col width = "200">
    <col width = "100">
    <col width = "100">
    <col width = "100">
    <tr> 
        <td> <b> Classifier </b> </td><td><b> Accuracy </b></td><td><b> Precision </b></td><td><b> Recall </b></td>
    </tr>
    <tr> 
        <td> Text Only (Base Line) </td><td> .785924 </td><td> .818 </td><td> .606</td>
    </tr>
    <tr> 
        <td> Temporal </td><td> .798039 </td><td> .831 </td><td> .63</td>
    </tr>
    <tr> 
        <td> (Social) User Features </td><td> .797059 </td><td> .824 </td><td> .635</td>
    </tr>
    <tr> 
        <td> (Social) Tweet Features </td><td> .812745 </td><td> .805 </td><td> .71</td>
    </tr>
    <tr> 
        <td> All </td><td> .824510 </td><td> .829 </td><td> .715</td>
    </tr>
</table>

<h2>Location Resolution</h2>
<p>GPS tagged tweets are easy. Otherwise some users provide a city or state level location string in a profile, and we simply map to the center of the location. If the tweet happens to contain something that looks like an address we can pick up with a regex we use that instead.</p>
<h2>Ranking Results</h2>
<p>In order to present important tweets first in the UI we took a simple approach that proved to be effective. Pagerank like algorithms aren't practical because links between tweets aren't nessacarily common nor indicative of importance. Similarly an approach based on users connections can measure the authority of a source, but this alone is also a poor indicator of importance. 
We chose a learn a function via linear regression that assigns a real number rank to each crime related tweet based on a similar set of features we used in general classification. This combines learned importance of certain keywords, use authority, and tweet spread.</p>
<h2>Implementation</h2>
<p>We implemented TEDAS based on Java and PHP with support of MySQL, Lucene, Twitter API, and Google Maps API. At the time of writing, the system has indexed
over two million CDE tweets and about a million usersâ€™ information, and continues to index at rate of about 30,000 new CDE tweets per day. It supports detecting events related to crimes, accidents, and disasters.</p>
		</div> <!--/#entry-content-->
			</div> <!--/#main-->
</div>  <!--/#post-->		<div class="navigation">
		</div>
		</div>
		
		<div id="footer">
			<p>Powered by <a href="http://pelican.readthedocs.org">Pelican</a>, theme by <a href="http://bunnyman.info">tBunnyMan</a>.</p>
							<script type="text/javascript">
					var _gaq = _gaq || [];
					_gaq.push(['_setAccount', 'UA-39357717-1']);
					_gaq.push(['_trackPageview']);
					(function() {
						var ga = document.createElement('script'); 
						ga.type = 'text/javascript'; ga.async = true;
						ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
						var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
					})();
				</script>
										<script type="text/javascript">
    var disqus_shortname = 'ramblesaurus';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>					</div><!-- /#footer -->
	</div><!-- /#container -->
	<div style="display:none"></div>
</body>
</html>