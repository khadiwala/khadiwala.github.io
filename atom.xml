<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Ramblesaurus]]></title>
  <link href="http://mrgrieves.github.com/atom.xml" rel="self"/>
  <link href="http://mrgrieves.github.com/"/>
  <updated>2013-03-30T10:44:41-05:00</updated>
  <id>http://mrgrieves.github.com/</id>
  <author>
    <name><![CDATA[Ravi Khadiwala]]></name>
    <email><![CDATA[ravi.khadiwala@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Rubiks Cube Solver in Go]]></title>
    <link href="http://mrgrieves.github.com/blog/2013/03/28/rubiks-cube-solver-in-go/"/>
    <updated>2013-03-28T23:55:00-05:00</updated>
    <id>http://mrgrieves.github.com/blog/2013/03/28/rubiks-cube-solver-in-go</id>
    <content type="html"><![CDATA[<h1>Shut up about Go</h1>

<p>Despite being a few years after its initial release, I can&#8217;t stop hearing about Go.
I decided to rework a simple enough algorithm I understood to learn basic Go syntax, and see how it performs in the memory and speed departments.
I implemented Thistlethwaite&#8217;s 4 phase algorithm for solving the Rubik&#8217;s Cube. I drew most of my inspiration from this fairly nice C++ <a href="http://tomas.rokicki.com/cubecontest/stefan1.txt">implementation</a> I found from Stefan Pochmann.</p>

<h1>Thistlethwaite&#8217;s Algorithm</h1>

<p>The basic idea is that the search for a solved cube is broken into four smaller sequential searches. Once in a phase we limit our moves to ensure we don&#8217;t exit this phase. A set of moves that can be applied to a solved cube defines a phase, the 4 phases are nested, the last one being a solved cube.</p>

<p>G<sub> 0 </sub> = &lt; U D L R F B ></p>

<p>G<sub>1</sub> = &lt; U2 D2 L R F B ></p>

<p>G<sub>2</sub> = &lt; U2 D2 L R F2 B2 ></p>

<p>G<sub>3</sub> = &lt; U2 D2 L2 R2 F2 B2 ></p>

<p>G<sub>4</sub> = &lt;></p>

<p>Basically a state is in phase i if, starting at the completed state, you can reach this state only by using moves in G<sub>i</sub>. I used iterative deepening DFS as opposed to BFS to save on memory at the cost of computation, since when I tried to use straight BFS I would run out of ram. At somepoint I may rewrite the cube structure to be easier on memory so I can go back to normal BFS. Another thing I&#8217;d like to do is to find a simpler representation of the cube that a human could input, instead of being forced to also input the orientations of edges and corner, but still be able to determine the phase easily based on this representation. In general the solver takes 30-40 moves to solve a randomly scrambled cube.</p>

<p><a href="http://www.jaapsch.net/puzzles/thistle.htm">Thistlethwaite&#8217;s original description</a></p>

<p><a href="http://www.jaapsch.net/puzzles/compcube.htm#kocal">Kociemba&#8217;s faster two phase algorithm</a></p>

<h1>Thoughts on Go</h1>

<p>Overall Go is without competition the most fun I&#8217;ve had with a &#8220;systems&#8221; programming language. The type inference is awesome, there are some powerful abstractions like slices, and go routines and channels are a killer language feature (which of course I did not use much of here). I think Go also captures what is most useful about object oriented programming with interfaces, without getting bogged down and forcing it on you, although the syntax is not super clear at first. I am sometimes uncomfortable with the occasional abstraction of pointers, like the automatic type conversion when you call a method for a struct its pointer. It only adds confusion, especially because you shouldn&#8217;t be depending on Go for this in most cases. Most of the other things I did not enjoy were nitpicks:</p>

<ul>
<li><p>Compilation errors for unused variables - makes it annoying to debug pieces of code by commenting things out</p></li>
<li><p>Anonymous recursive functions must be preceded by a type definition</p></li>
<li><p>Must include return statements in functions even if unreachable</p></li>
</ul>


<p>My only real philisophical complaint revolve around there being a little too much magic to be a low level language, but not enough to prevent you from having to think about internals.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TEDAS]]></title>
    <link href="http://mrgrieves.github.com/blog/2013/03/16/tedas/"/>
    <updated>2013-03-16T21:07:00-05:00</updated>
    <id>http://mrgrieves.github.com/blog/2013/03/16/tedas</id>
    <content type="html"><![CDATA[<h1>Twitter based Event Detection and Analysis System</h1>

<p>Twitter is a distributed, fast, and localized system for spreading information. It&#8217;s also noisy, inaccurate, and completly disorganized.
Discovery tweets that are related to important events, for example crimes and natural disasters, and providing georgraphical context could be a useful way to propogate important news at speeds that traditional media cannot accomplish.
A demo of TEDAS is avaliable <a href="http://canary.cs.illinois.edu/crimedetection/index_ui.php">here</a>, with an assoiciated <a href="http://mias.illinois.edu/files/Twitter%20Event%20Detection%20demo%20paper.pdf">paper</a> from icde 2012.</p>

<h2>Crawling and Classification</h2>

<p>We began with a small set of seed keywords that brought in a reasonable proportion of crime related tweets. We manually labeled a few thousand of these tweets (oh god it was horrible), and had a clean dataset on which to base crawling and classification.
Based on the labeled data, keyword based textual features, and social features we created an SVM classifier. We then iterativley improve both crawling and classification by revaluating the proportion of relevant tweets that a keyword captures, removing keywords that bring in a high proportion of non event related tweets and adding keywords that are present in a high number of event related tweets. This is important because Twitter limits the number of tweets returned by its API, thus keywords that bring a lot of noise waste precious requests. Furthermore if a keyword becomes more relevant to important events or becomes co-opted by some annoying pop culture event, the system can adapt. Here is an example of finding new potential rules based on seed keywords:</p>

<table align="center"> 
<col width = "100">
<col width = "100">
        <tr>
            <td> <b> Seeds </b> </td>
            <td> <b> Discoveries </b> </td>
        </tr>
        <tr>
            <td> investigate </td>
            <td> ap breaking </td>
        </tr>
        <tr>
            <td> robbery </td>
            <td> word from </td>
        </tr>
        <tr>
            <td> arrest</td>
            <td> demonstrators </td>
        </tr> 
        <tr>
            <td> officier </td>
            <td> adventure </td>
        </tr> 
        <tr>
            <td></td>
            <td> riot </td>
        </tr>
        <tr>
            <td></td>
            <td> violate arrest </td>
        </tr>
        <tr>
            <td></td>
            <td> abducted </td>
        </tr>
        <tr>
            <td></td>
            <td> wasn&#8217;t carrying drugs </td>
        </tr>
        <tr>
            <td></td>
            <td> vehicle </td>
        </tr>
</table>


<p>You can see that the discovery is highly dependant on popular subjects at the time of crawling.</p>

<p>A unique benefit to Twitter is the presence of social features. Number of followers, hashtags, retweets, all serve as useful indicators of the accuracy and relevance of tweets. Especially since traditional text based classification techniques may falter due to the short character limit of tweets (140) and the unique language tweets can take. We also can consider temporal and spatial features, like if we see several tweets at the same time and location with similar content we can infer that an important event is happening there.</p>

<table>
    <col width = "200">
    <col width = "100">
    <col width = "100">
    <col width = "100">
    <tr> 
        <td> <b> Classifier </b> </td><td><b> Accuracy </b></td><td><b> Precision </b></td><td><b> Recall </b></td>
    </tr>
    <tr> 
        <td> Text Only (Base Line) </td><td> .785924 </td><td> .818 </td><td> .606</td>
    </tr>
    <tr> 
        <td> Temporal </td><td> .798039 </td><td> .831 </td><td> .63</td>
    </tr>
    <tr> 
        <td> (Social) User Features </td><td> .797059 </td><td> .824 </td><td> .635</td>
    </tr>
    <tr> 
        <td> (Social) Tweet Features </td><td> .812745 </td><td> .805 </td><td> .71</td>
    </tr>
    <tr> 
        <td> All </td><td> .824510 </td><td> .829 </td><td> .715</td>
    </tr>
</table>


<h2>Location Resolution</h2>

<p>GPS tagged tweets are easy. Otherwise some users provide a city or state level location string in a profile, and we simply map to the center of the location. If the tweet happens to contain something that looks like an address we can pick up with a regex we use that instead.</p>

<h2>Ranking Results</h2>

<p>In order to present important tweets first in the UI we took a simple approach that proved to be effective. Pagerank like algorithms aren&#8217;t practical because links between tweets aren&#8217;t nessacarily common nor indicative of importance. Similarly an approach based on users connections can measure the authority of a source, but this alone is also a poor indicator of importance.
We chose a learn a function via linear regression that assigns a real number rank to each crime related tweet based on a similar set of features we used in general classification. This combines learned importance of certain keywords, use authority, and tweet spread.</p>

<h2>Implementation</h2>

<p>We implemented TEDAS based on Java and PHP with support of MySQL, Lucene, Twitter API, and Google Maps API. At the time of writing, the system has indexed
over two million CDE tweets and about a million usersâ€™ information, and continues to index at rate of about 30,000 new CDE tweets per day. It supports detecting events related to crimes, accidents, and disasters.</p>
]]></content>
  </entry>
  
</feed>
